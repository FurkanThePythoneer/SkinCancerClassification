# NOTE: it is best to use absolute paths
# If you must use relative paths, they are relative
# to run.py, not the config file

seed: 88 

dataset:
  name: SkinDataset
  params:
    flip: true
    verbose: true
  sampler:
    name: BalancedSampler
    params:
      weights:  [0.75, 0.25]
      pos_label: 1 
  inner_fold: 0
  outer_fold: 0
  outer_only: true
  data_dir: ../data/jpeg/train/
  csv_filename: ../data/train_with_splits.csv


transform:
  augment: grid_mask
  params:
    k: [0.500, 0.75]
    D: [0.375, 1.00]
    p_start: 0.0
    p_end: 0.8
    always_apply: false
  num_workers: 0
  pad_ratio: 1.0
  resize_to: [512, 512]
  preprocess:
    image_range: [0, 255]
    input_range: [0, 1]
    mean: [0.485, 0.456, 0.406]
    sdev: [0.229, 0.224, 0.225]


model:
  name: ArcNet
  params:
    backbone: tf_efficientnet_b4
    pretrained: true
    num_classes: 2
    dropout: 0.2
    pool: gem


find_lr: # this is its own mode 
  params:
    start_lr: 1.0e-7
    end_lr: 1
    num_iter: 500
    save_fig: true


train:
  load_previous: ../checkpoints/isic2019/mk001/EFFB4_019_VM-0.9927.PTH
  batch_size: 16
  trainer: Trainer
  params:
    gradient_accumulation: 1
    num_epochs: 20
    steps_per_epoch: 0
    validate_interval: 2
    verbosity: 10
    amp: true
    grid_mask: 0.8


evaluation:
  batch_size: 8
  evaluator: Evaluator
  params:
    save_checkpoint_dir: ../checkpoints/mk002/
    save_best: true
    prefix: effb4
    metrics: [arc_auc]
    valid_metric: arc_auc
    mode: max
    improve_thresh: 1.0e-6


loss:
  name: ArcFaceLoss
  params:


optimizer:
  name: AdamW
  params:
    lr: 1.2e-5
    weight_decay: 5.0e-4


scheduler:
  name: CustomOneCycleLR
  params:
    max_lr:   3.0e-4
    final_lr: 1.0e-12
    pct_start: 0.3


test:
  checkpoint:
  batch_size: 128
  data_dir:
  save_preds_dir: 
  labels_available: 
  outer_only: True
